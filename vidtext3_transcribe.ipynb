{"cells":[{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-04-13T22:04:59.508826Z","iopub.status.busy":"2024-04-13T22:04:59.508412Z","iopub.status.idle":"2024-04-13T22:05:11.674168Z","shell.execute_reply":"2024-04-13T22:05:11.672810Z","shell.execute_reply.started":"2024-04-13T22:04:59.508797Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: pytube in /opt/conda/lib/python3.10/site-packages (15.0.0)\n","Requirement already satisfied: audio_extract in /opt/conda/lib/python3.10/site-packages (0.6.0)\n","Requirement already satisfied: pydub in /opt/conda/lib/python3.10/site-packages (0.25.1)\n","Requirement already satisfied: ffmpeg-python==0.2.0 in /opt/conda/lib/python3.10/site-packages (from audio_extract) (0.2.0)\n","Requirement already satisfied: imageio-ffmpeg==0.4.8 in /opt/conda/lib/python3.10/site-packages (from audio_extract) (0.4.8)\n","Requirement already satisfied: mutagen==1.46.0 in /opt/conda/lib/python3.10/site-packages (from audio_extract) (1.46.0)\n","Requirement already satisfied: future in /opt/conda/lib/python3.10/site-packages (from ffmpeg-python==0.2.0->audio_extract) (1.0.0)\n"]}],"source":["! pip install pytube audio_extract pydub"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-04-13T22:05:20.582346Z","iopub.status.busy":"2024-04-13T22:05:20.581955Z","iopub.status.idle":"2024-04-13T22:05:20.602144Z","shell.execute_reply":"2024-04-13T22:05:20.601423Z","shell.execute_reply.started":"2024-04-13T22:05:20.582310Z"},"trusted":true},"outputs":[],"source":["from pytube import YouTube\n","from pydub import AudioSegment\n","import torch\n","from audio_extract import extract_audio\n","from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, AutoModelForSeq2SeqLM, AutoProcessor, AutoModelForSpeechSeq2Seq"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-04-13T22:04:25.466049Z","iopub.status.busy":"2024-04-13T22:04:25.465345Z","iopub.status.idle":"2024-04-13T22:04:25.494354Z","shell.execute_reply":"2024-04-13T22:04:25.493276Z","shell.execute_reply.started":"2024-04-13T22:04:25.466010Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["0\n"]}],"source":["audio_model_id = \"openai/whisper-large-v3\"\n","device = 0 if torch.cuda.is_available() else \"cpu\"\n","print(device)\n","# processor = AutoProcessor.from_pretrained(\"openai/whisper-large-v3\")\n","# audio_model = AutoModelForSpeechSeq2Seq.from_pretrained(\"openai/whisper-large-v3\")"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-04-13T22:04:25.498590Z","iopub.status.busy":"2024-04-13T22:04:25.497359Z","iopub.status.idle":"2024-04-13T22:04:25.729447Z","shell.execute_reply":"2024-04-13T22:04:25.728449Z","shell.execute_reply.started":"2024-04-13T22:04:25.498554Z"},"trusted":true},"outputs":[],"source":["\n","# https://youtube.com/shorts/FJtFZwbvkI4?si=EmT9MhTuRvP26FzA\n","\n","url = \"https://youtu.be/ORMx45xqWkA?si=-CDrDOzt6hC13n8k\"\n","\n","video = YouTube(url)\n","\n","title = video.title\n","\n","output_file = f'{title}.mp4'"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-04-13T22:04:25.731421Z","iopub.status.busy":"2024-04-13T22:04:25.730986Z","iopub.status.idle":"2024-04-13T22:04:28.261807Z","shell.execute_reply":"2024-04-13T22:04:28.260877Z","shell.execute_reply.started":"2024-04-13T22:04:25.731362Z"},"trusted":true},"outputs":[],"source":["video_dld = video.streams.filter(progressive=True, file_extension=\"mp4\").order_by(\"resolution\").desc().first().download()"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-04-13T22:04:28.264887Z","iopub.status.busy":"2024-04-13T22:04:28.264489Z","iopub.status.idle":"2024-04-13T22:04:32.211623Z","shell.execute_reply":"2024-04-13T22:04:32.210503Z","shell.execute_reply.started":"2024-04-13T22:04:28.264845Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Success : audio file has been saved to \"/kaggle/working/PyTorch in 100 Seconds.mp3\".\n"]}],"source":["audio_file = extract_audio(input_path=video_dld, output_path=f'{title}.mp3')"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-04-13T22:08:09.841610Z","iopub.status.busy":"2024-04-13T22:08:09.840764Z","iopub.status.idle":"2024-04-13T22:08:10.320658Z","shell.execute_reply":"2024-04-13T22:08:10.319637Z","shell.execute_reply.started":"2024-04-13T22:08:09.841577Z"},"trusted":true},"outputs":[],"source":["file = \"/kaggle/working/PyTorch in 100 Seconds.mp3\"\n","audio = AudioSegment.from_file(file, format=\"mp3\")\n","\n","wav_file = 'audio_file.wav'\n","\n","audio_sample = audio.export(wav_file, format=\"wav\")"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-04-13T22:09:59.268823Z","iopub.status.busy":"2024-04-13T22:09:59.268357Z","iopub.status.idle":"2024-04-13T22:14:11.516549Z","shell.execute_reply":"2024-04-13T22:14:11.515459Z","shell.execute_reply.started":"2024-04-13T22:09:59.268788Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n"]},{"name":"stdout","output_type":"stream","text":["PyTorch in 100 Seconds\n","{'text': \" PyTorch, an open-source deep learning framework used to build some of the world's most famous artificial intelligence products. It was created at the Meta AI Research Lab in 2016, but is actually derived from the Lua-based Torch library that dates back to 2002. Fundamentally, it's a library for programming with tensors, which are basically just multidimensional arrays that represent data and parameters in deep neural networks. Sounds complicated, but its focus on usability will have you training machine learning models with just a few lines of Python. In addition, it facilitates high-performance parallel computing on a GPU thanks to NVIDIA's CUDA platform. Developers love prototyping with it because it supports a dynamic computational graph, allowing models to be optimized at runtime. It does this by constructing a directed acyclic graph consisting of functions that keeps track of all the executed operations on the tensors, allowing you to change the shape, size, and operations after every iteration if needed. PyTorch has been used to train models for computer vision AI like Tesla Autopilot, image generators like Stable Diffusion, and speech recognition models like OpenAI Whisper, just to name a few. To get started, install PyTorch and optionally CUDA if you want to accelerate computing on your GPU. Now import it into a Python file or notebook. Like I mentioned, a tensor is similar to a multidimensional array. Create a 2D array or matrix with Python, then use Torch to convert it into a tensor. Now we can run all kinds of computations on it, like we might convert all these integers into random floating points. We can also perform linear algebra by taking multiple tensors and multiplying them together. What you came here to do though is build a deep neural network, like an image classifier. To handle that, we can define a new class that inherits from the neural network module class. Inside the constructor, we can build it out layer by layer. The flattened layer will take a multidimensional input, like an image, and convert it to one dimension. From there, sequential is used to create a container of layers that the data will flow through. Each layer has multiple nodes, where each node is like its own mini-statistical model. As each data point flows through it, it'll try to guess the output, and gradually update a mapping of weights to determine the importance of a given variable. Linear is a fully connected layer that takes the flattened 28x28 image and transforms it to an output of 512. This layer is followed by a non-linear activation function. When activated, it means that feature might be important and outputs the node, otherwise it just outputs 0. And finally, we finish with a fully connected layer that outputs the 10 labels the model is trying to predict. With these pieces in place, the next step is to define a forward method that describes the flow of data, and now instantiate the model to a GPU and pass it some input data. This will automatically call its forward method for training and prediction. Congratulations, you just built a neural network. This has been PyTorch in 100 seconds. Thanks for watching, and I will see you in the next one.\"}\n"]}],"source":["transcriber = pipeline(task=\"automatic-speech-recognition\", model=\"openai/whisper-large-v3\")\n","\n","text_extract = transcriber(wav_file)\n","print(title)\n","print(text_extract)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30683,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
